{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b67e9bff-db75-43e1-8cca-1163f1724769",
   "metadata": {},
   "source": [
    "# Text Generation with Python and Keras\n",
    "\n",
    "## Part One\n",
    "\n",
    "1. Read Moby Dick .txt files into pandas\n",
    "2. Process Text\n",
    "3. Clean Text\n",
    "4. Tokenize the Text and create Sequences with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bab26109-ce33-4358-98bc-6feef020005d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to read in a .txt file\n",
    "# Once definition is written, a .txt file with four chapter of moby dick will appear below\n",
    "\n",
    "def read_file(filepath):\n",
    "    with open(filepath) as f:\n",
    "        str_text = f.read()\n",
    "        \n",
    "    return str_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41e7fa31-9bc4-4399-a371-ab89d79505c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "read_file('moby_dick_four_chapters.txt') # passing in the four chapters of moby dick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c54c445e-7ca6-4804-bb4b-551e87158709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing spacy library to tokenize text\n",
    "# we will also disable any parts of th\n",
    "\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_md', disable = ['parser', 'tagger', 'ner'])\n",
    "nlp.max_length = 1198623 # setting the max words to be greater than one million. this should cover the entire four chapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5dbb838b-ea6c-442a-9899-092ca684798e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a function to take in a string and grab the tokens if they are not a punctuation or a new line (\\n\\n) \n",
    "\n",
    "def separate_punc(doc_text):\n",
    "    return [token.text.lower() for token in nlp(doc_text) if token.text not in '\\n\\n \\n\\n\\n!\"-#$%&()--.*+,-/:;<=>?@[\\\\]^_`{|}~\\t\\n']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4f31c33-0175-4593-a9cd-d73f813dd340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing the file into a variable\n",
    "\n",
    "d = read_file('moby_dick_four_chapters.txt')\n",
    "\n",
    "# putting the moby dick chapters through the separate_punc function\n",
    "tokens = separate_punc(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c9100319-7550-4d22-b011-5adb0129bec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens # a token is a string with a known meaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c9d6f38-c9cf-41fd-8628-95cc3fcc0c17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11338"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens) # the length of tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ae13e2-48ca-4cd3-b7bb-dadd702d7fe0",
   "metadata": {},
   "source": [
    "# Creating a Sequence of Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a8bdc5e9-0512-4c4a-adea-252bd43237cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# organize into sequences of tokens\n",
    "train_len = 25+1 # 50 training words, then one target word\n",
    "\n",
    "# Creating an empty list of sequences\n",
    "text_sequences = []\n",
    "\n",
    "for i in range(train_len, len(tokens)): # range is the training length, up to the length of all of the tokens \n",
    "    \n",
    "    # grab the amount of characters in train_len\n",
    "    seq = tokens[i-train_len:i] # i minus train_len up to i \n",
    "    \n",
    "    # Add to text_sequences using append\n",
    "    text_sequences.append(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc124af5-6bf8-4cab-8463-3bfb5cb479a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'call me ishmael some years ago never mind how long precisely having little or no money in my purse and nothing particular to interest me on'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(text_sequences[0]) # joining the tokens together to form a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "51e2b86e-4567-4446-8607-d15ced5250a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'me ishmael some years ago never mind how long precisely having little or no money in my purse and nothing particular to interest me on shore'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(text_sequences[1]) # moves one word over to the right - as we can see it started with call me and now it starts with me"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8a500075-785a-4c3c-bfd5-415e10832a6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ishmael some years ago never mind how long precisely having little or no money in my purse and nothing particular to interest me on shore i'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(text_sequences[2]) # on this sequence again it moved one word over to the right, starting with ishmael"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1abe8837-4793-468a-9add-cd7700d3715c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11312"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b274272b-0ae2-4e1d-9a10-ad829d21995e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing keras and the tokenizer\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8c31de1d-7477-4045-9088-2b948ef3a49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# integer encode sequences of words\n",
    "# the number is an id for the word - as it is unique to each word\n",
    "\n",
    "tokenizer = Tokenizer() # create the tokenizer object\n",
    "tokenizer.fit_on_texts(text_sequences) # calling on tokenizers fit_on_texts and provide the text_sequences\n",
    "sequences = tokenizer.texts_to_sequences(text_sequences) # calling on texts_to_sequences and replaces texts sequences to sequences of numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4d9aa7a9-34ca-4543-a45d-0b837468c52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences[0] # the words are now represented by numbers or id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5dc46620-ea92-4f9b-8ce2-f47bf6299c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.index_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5c7703fe-87ed-42ba-8c05-cc402b192ef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "956 : call\n",
      "14 : me\n",
      "263 : ishmael\n",
      "51 : some\n",
      "261 : years\n",
      "408 : ago\n",
      "87 : never\n",
      "219 : mind\n",
      "129 : how\n",
      "111 : long\n",
      "954 : precisely\n",
      "260 : having\n",
      "50 : little\n",
      "43 : or\n",
      "38 : no\n",
      "314 : money\n",
      "7 : in\n",
      "23 : my\n",
      "546 : purse\n",
      "3 : and\n",
      "150 : nothing\n",
      "259 : particular\n",
      "6 : to\n",
      "2713 : interest\n",
      "14 : me\n",
      "24 : on\n"
     ]
    }
   ],
   "source": [
    "# for i in sequences, print out the id : followed by word\n",
    "for i in sequences[0]:\n",
    "    print(f'{i} : {tokenizer.index_word[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "28d49de8-4b4a-4d13-b632-8437cd8cf3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.word_counts # counts how many times each words shows up ie. Ishmael shows up 133 times in this .txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d42704d7-f896-4cb9-b63f-a1ea61eeb850",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = len(tokenizer.word_counts) # the length of the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "437e61cf-3d85-415e-af91-ee50e8f314f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing numpy\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dcb72652-8c10-4bab-8d5c-108eda68c48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = np.array(sequences) # transforming sequences the list into a numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bc489934-6522-491d-99b3-4afe236355ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 956,   14,  263, ..., 2713,   14,   24],\n",
       "       [  14,  263,   51, ...,   14,   24,  957],\n",
       "       [ 263,   51,  261, ...,   24,  957,    5],\n",
       "       ...,\n",
       "       [ 952,   12,  166, ...,  262,   53,    2],\n",
       "       [  12,  166, 2712, ...,   53,    2, 2718],\n",
       "       [ 166, 2712,    3, ...,    2, 2718,   26]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# last word on the right - ie. 24 in the first row is the target word or in other words the label\n",
    "# the features would be the 25 numbers starting from 956 and ending at 14\n",
    "\n",
    "sequences # formatted sequences into the numpy array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709f632c-d55a-48b3-b139-92a6a977a412",
   "metadata": {},
   "source": [
    "# Creating an LSTM based model\n",
    "\n",
    "## Approach\n",
    "\n",
    "+ Create the LSTM based model\n",
    "+ Split the data into features and labels\n",
    "\n",
    "  - X Features (First 25 words of Sequence)\n",
    "  - Y Label (Next word after the sequence)\n",
    "\n",
    "\n",
    "+ Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "00ef358b-257f-4e67-8548-553fb71ea887",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "76ef458b-bda1-4eb7-9acf-ec563b87992d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(vocabulary_size, seq_len):\n",
    "    model = Sequential() # creating an instance of a sequential model\n",
    "    model.add(Embedding(vocabulary_size, 25, input_length = seq_len)) # transforming into fixed size, allowing us to fit text data\n",
    "    model.add(LSTM(150, return_sequences = True)) # number of units or neurons  \n",
    "    model.add(LSTM(150)) # adding a second layer of LSTM\n",
    "    model.add(Dense(150, activation = 'relu')) # relu = rectified linear activation function. Output directly if it is positive or else it will output zero.\n",
    "    \n",
    "    model.add(Dense(vocabulary_size, activation = 'softmax')) \n",
    "    \n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34f5e5f-91fa-4585-953f-4ccb93008e0c",
   "metadata": {},
   "source": [
    "# Train / Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "91d40530-f193-4547-84a4-5bd465f9023b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical # converts a class vector (integers) to binary class matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5eed86ba-6a7e-405a-9411-77de942b5233",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 956,   14,  263, ..., 2713,   14,   24],\n",
       "       [  14,  263,   51, ...,   14,   24,  957],\n",
       "       [ 263,   51,  261, ...,   24,  957,    5],\n",
       "       ...,\n",
       "       [ 952,   12,  166, ...,  262,   53,    2],\n",
       "       [  12,  166, 2712, ...,   53,    2, 2718],\n",
       "       [ 166, 2712,    3, ...,    2, 2718,   26]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3a7e1746-f50a-4c6b-a467-d2bb5b294bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First 49 words\n",
    "\n",
    "X = sequences[:,:-1] # Features - this gets the first 49 words and excludes the last which is y or the label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6fa673ff-fd10-4660-b782-e7ad95d37aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = sequences[:,-1] # Labels - last word and the word we are trying to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "58f54243-4c02-41c1-9fff-ae0deb924f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = to_categorical(y, num_classes = vocabulary_size+1) # the way keras padding words it needs an extra 1 to hold 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4d6cd645-a642-4223-b83a-fed59aa185d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0a503657-f0ef-4046-94fa-1136d890957a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_len"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d4fe96-4bb8-4fde-8162-de048dba1566",
   "metadata": {},
   "source": [
    "# Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e66b8d71-d6f8-48c8-80d1-80c8b611aa10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 25, 25)            67975     \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 25, 150)           105600    \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 150)               180600    \n",
      "                                                                 \n",
      " dense (Dense)               (None, 150)               22650     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2719)              410569    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 787,394\n",
      "Trainable params: 787,394\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# defining the model\n",
    "\n",
    "model = create_model(vocabulary_size+1, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6a130a96-af78-4a80-b06c-f02e41ee1376",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import dump,load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "389816c2-279e-4a2e-852c-b73a1e47c0f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/350\n",
      "89/89 [==============================] - 7s 51ms/step - loss: 6.8423 - accuracy: 0.0520\n",
      "Epoch 2/350\n",
      "89/89 [==============================] - 5s 53ms/step - loss: 6.3863 - accuracy: 0.0529\n",
      "Epoch 3/350\n",
      "89/89 [==============================] - 5s 53ms/step - loss: 6.3455 - accuracy: 0.0529\n",
      "Epoch 4/350\n",
      "89/89 [==============================] - 5s 52ms/step - loss: 6.2042 - accuracy: 0.0519\n",
      "Epoch 5/350\n",
      "89/89 [==============================] - 5s 53ms/step - loss: 6.1146 - accuracy: 0.0535\n",
      "Epoch 6/350\n",
      "89/89 [==============================] - 5s 53ms/step - loss: 5.9976 - accuracy: 0.0639\n",
      "Epoch 7/350\n",
      "89/89 [==============================] - ETA: 0s - loss: 5.8794 - accuracy: 0.06 - 5s 53ms/step - loss: 5.8800 - accuracy: 0.0671\n",
      "Epoch 8/350\n",
      "89/89 [==============================] - 5s 54ms/step - loss: 5.7975 - accuracy: 0.0682\n",
      "Epoch 9/350\n",
      "89/89 [==============================] - 5s 53ms/step - loss: 5.7256 - accuracy: 0.0713\n",
      "Epoch 10/350\n",
      "89/89 [==============================] - 5s 53ms/step - loss: 5.6513 - accuracy: 0.0738\n",
      "Epoch 11/350\n",
      "89/89 [==============================] - 5s 52ms/step - loss: 5.5841 - accuracy: 0.0764 0s - loss: 5.5810 - accuracy\n",
      "Epoch 12/350\n",
      "89/89 [==============================] - 5s 52ms/step - loss: 5.5244 - accuracy: 0.0778\n",
      "Epoch 13/350\n",
      "89/89 [==============================] - 5s 52ms/step - loss: 5.4724 - accuracy: 0.0818\n",
      "Epoch 14/350\n",
      "89/89 [==============================] - 5s 52ms/step - loss: 5.4203 - accuracy: 0.0843\n",
      "Epoch 15/350\n",
      "89/89 [==============================] - 5s 52ms/step - loss: 5.3704 - accuracy: 0.0865\n",
      "Epoch 16/350\n",
      "89/89 [==============================] - 5s 52ms/step - loss: 5.3272 - accuracy: 0.0899\n",
      "Epoch 17/350\n",
      "89/89 [==============================] - 5s 52ms/step - loss: 5.2789 - accuracy: 0.0888\n",
      "Epoch 18/350\n",
      "89/89 [==============================] - 5s 52ms/step - loss: 5.2298 - accuracy: 0.0903\n",
      "Epoch 19/350\n",
      "89/89 [==============================] - 5s 52ms/step - loss: 5.1875 - accuracy: 0.0901\n",
      "Epoch 20/350\n",
      "89/89 [==============================] - 5s 52ms/step - loss: 5.1450 - accuracy: 0.0914\n",
      "Epoch 21/350\n",
      "89/89 [==============================] - 5s 52ms/step - loss: 5.0966 - accuracy: 0.0912\n",
      "Epoch 22/350\n",
      "89/89 [==============================] - 5s 52ms/step - loss: 5.0525 - accuracy: 0.0934\n",
      "Epoch 23/350\n",
      "89/89 [==============================] - 5s 52ms/step - loss: 5.0079 - accuracy: 0.0935\n",
      "Epoch 24/350\n",
      "89/89 [==============================] - 5s 52ms/step - loss: 4.9715 - accuracy: 0.0961\n",
      "Epoch 25/350\n",
      "89/89 [==============================] - 5s 52ms/step - loss: 4.9157 - accuracy: 0.0949\n",
      "Epoch 26/350\n",
      "89/89 [==============================] - 5s 52ms/step - loss: 4.8733 - accuracy: 0.0993\n",
      "Epoch 27/350\n",
      "89/89 [==============================] - 5s 52ms/step - loss: 4.8299 - accuracy: 0.0995\n",
      "Epoch 28/350\n",
      "89/89 [==============================] - 5s 52ms/step - loss: 4.7768 - accuracy: 0.1005\n",
      "Epoch 29/350\n",
      "89/89 [==============================] - 5s 52ms/step - loss: 4.7300 - accuracy: 0.1045\n",
      "Epoch 30/350\n",
      "89/89 [==============================] - 5s 52ms/step - loss: 4.6799 - accuracy: 0.1082\n",
      "Epoch 31/350\n",
      "89/89 [==============================] - 5s 51ms/step - loss: 4.6363 - accuracy: 0.1085\n",
      "Epoch 32/350\n",
      "89/89 [==============================] - 5s 52ms/step - loss: 4.6002 - accuracy: 0.1089\n",
      "Epoch 33/350\n",
      "89/89 [==============================] - 5s 52ms/step - loss: 4.5490 - accuracy: 0.1110\n",
      "Epoch 34/350\n",
      "89/89 [==============================] - 5s 52ms/step - loss: 4.5126 - accuracy: 0.1127\n",
      "Epoch 35/350\n",
      "89/89 [==============================] - 5s 52ms/step - loss: 4.8968 - accuracy: 0.1005\n",
      "Epoch 36/350\n",
      "89/89 [==============================] - 5s 52ms/step - loss: 4.8477 - accuracy: 0.0975\n",
      "Epoch 37/350\n",
      "89/89 [==============================] - 5s 52ms/step - loss: 4.6129 - accuracy: 0.1048\n",
      "Epoch 38/350\n",
      "89/89 [==============================] - 5s 52ms/step - loss: 4.4940 - accuracy: 0.1123\n",
      "Epoch 39/350\n",
      "89/89 [==============================] - 5s 52ms/step - loss: 4.4009 - accuracy: 0.1164\n",
      "Epoch 40/350\n",
      "89/89 [==============================] - 5s 52ms/step - loss: 4.3294 - accuracy: 0.1173\n",
      "Epoch 41/350\n",
      "89/89 [==============================] - 5s 52ms/step - loss: 4.2573 - accuracy: 0.1213\n",
      "Epoch 42/350\n",
      "89/89 [==============================] - 5s 52ms/step - loss: 4.1912 - accuracy: 0.1287\n",
      "Epoch 43/350\n",
      "89/89 [==============================] - 5s 52ms/step - loss: 4.1313 - accuracy: 0.1281\n",
      "Epoch 44/350\n",
      "89/89 [==============================] - 5s 52ms/step - loss: 4.0761 - accuracy: 0.1322\n",
      "Epoch 45/350\n",
      "89/89 [==============================] - 5s 53ms/step - loss: 4.0385 - accuracy: 0.1335\n",
      "Epoch 46/350\n",
      "89/89 [==============================] - 5s 52ms/step - loss: 4.1318 - accuracy: 0.1332\n",
      "Epoch 47/350\n",
      "89/89 [==============================] - 5s 54ms/step - loss: 4.0404 - accuracy: 0.1383\n",
      "Epoch 48/350\n",
      "89/89 [==============================] - 5s 54ms/step - loss: 3.9868 - accuracy: 0.1416\n",
      "Epoch 49/350\n",
      "89/89 [==============================] - 5s 52ms/step - loss: 3.9338 - accuracy: 0.1449\n",
      "Epoch 50/350\n",
      "89/89 [==============================] - 5s 52ms/step - loss: 3.8630 - accuracy: 0.1476\n",
      "Epoch 51/350\n",
      "89/89 [==============================] - 5s 52ms/step - loss: 3.8107 - accuracy: 0.1521\n",
      "Epoch 52/350\n",
      "89/89 [==============================] - 5s 52ms/step - loss: 3.7617 - accuracy: 0.1556\n",
      "Epoch 53/350\n",
      "89/89 [==============================] - 5s 52ms/step - loss: 3.6912 - accuracy: 0.1627\n",
      "Epoch 54/350\n",
      "89/89 [==============================] - 5s 52ms/step - loss: 3.6455 - accuracy: 0.1719\n",
      "Epoch 55/350\n",
      "89/89 [==============================] - 5s 52ms/step - loss: 3.5893 - accuracy: 0.1740\n",
      "Epoch 56/350\n",
      "89/89 [==============================] - 5s 52ms/step - loss: 3.5357 - accuracy: 0.1806\n",
      "Epoch 57/350\n",
      "89/89 [==============================] - 5s 52ms/step - loss: 3.4826 - accuracy: 0.1875\n",
      "Epoch 58/350\n",
      "89/89 [==============================] - 5s 52ms/step - loss: 3.4332 - accuracy: 0.1922\n",
      "Epoch 59/350\n",
      "89/89 [==============================] - 5s 52ms/step - loss: 3.3832 - accuracy: 0.2001\n",
      "Epoch 60/350\n",
      "89/89 [==============================] - 5s 52ms/step - loss: 3.3293 - accuracy: 0.2086\n",
      "Epoch 61/350\n",
      "89/89 [==============================] - 5s 52ms/step - loss: 3.2654 - accuracy: 0.2168\n",
      "Epoch 62/350\n",
      "89/89 [==============================] - 5s 52ms/step - loss: 3.2362 - accuracy: 0.2213\n",
      "Epoch 63/350\n",
      "89/89 [==============================] - 5s 52ms/step - loss: 3.3932 - accuracy: 0.2182\n",
      "Epoch 64/350\n",
      "89/89 [==============================] - 5s 53ms/step - loss: 3.3174 - accuracy: 0.2224\n",
      "Epoch 65/350\n",
      "89/89 [==============================] - 5s 52ms/step - loss: 3.2433 - accuracy: 0.2354\n",
      "Epoch 66/350\n",
      "89/89 [==============================] - 5s 52ms/step - loss: 3.1851 - accuracy: 0.2437\n",
      "Epoch 67/350\n",
      "89/89 [==============================] - 5s 53ms/step - loss: 3.3246 - accuracy: 0.2291\n",
      "Epoch 68/350\n",
      "89/89 [==============================] - 5s 54ms/step - loss: 3.6011 - accuracy: 0.1917\n",
      "Epoch 69/350\n",
      "89/89 [==============================] - 5s 54ms/step - loss: 3.5462 - accuracy: 0.1954\n",
      "Epoch 70/350\n",
      "89/89 [==============================] - 5s 55ms/step - loss: 3.3811 - accuracy: 0.2172\n",
      "Epoch 71/350\n",
      "89/89 [==============================] - 5s 55ms/step - loss: 3.3603 - accuracy: 0.2254\n",
      "Epoch 72/350\n",
      "89/89 [==============================] - 5s 55ms/step - loss: 3.3578 - accuracy: 0.2182\n",
      "Epoch 73/350\n",
      "89/89 [==============================] - 5s 54ms/step - loss: 3.2674 - accuracy: 0.2368\n",
      "Epoch 74/350\n",
      "89/89 [==============================] - 5s 54ms/step - loss: 3.2067 - accuracy: 0.2435\n",
      "Epoch 75/350\n",
      "89/89 [==============================] - 5s 53ms/step - loss: 3.1562 - accuracy: 0.2555\n",
      "Epoch 76/350\n",
      "89/89 [==============================] - 5s 52ms/step - loss: 3.1215 - accuracy: 0.2605\n",
      "Epoch 77/350\n",
      "89/89 [==============================] - 5s 52ms/step - loss: 3.0792 - accuracy: 0.2679\n",
      "Epoch 78/350\n",
      "89/89 [==============================] - 5s 55ms/step - loss: 2.9994 - accuracy: 0.2790\n",
      "Epoch 79/350\n",
      "89/89 [==============================] - 5s 53ms/step - loss: 3.2132 - accuracy: 0.2462\n",
      "Epoch 80/350\n",
      "89/89 [==============================] - 5s 53ms/step - loss: 3.3666 - accuracy: 0.2201\n",
      "Epoch 81/350\n",
      "89/89 [==============================] - 5s 53ms/step - loss: 3.2509 - accuracy: 0.2358\n",
      "Epoch 82/350\n",
      "89/89 [==============================] - 5s 53ms/step - loss: 3.0218 - accuracy: 0.2755\n",
      "Epoch 83/350\n",
      "89/89 [==============================] - 5s 53ms/step - loss: 2.8665 - accuracy: 0.2992\n",
      "Epoch 84/350\n",
      "89/89 [==============================] - 5s 53ms/step - loss: 2.7422 - accuracy: 0.3335\n",
      "Epoch 85/350\n",
      "89/89 [==============================] - 5s 57ms/step - loss: 2.6781 - accuracy: 0.3447\n",
      "Epoch 86/350\n",
      "89/89 [==============================] - 5s 56ms/step - loss: 2.6247 - accuracy: 0.3579\n",
      "Epoch 87/350\n",
      "89/89 [==============================] - 5s 55ms/step - loss: 2.5819 - accuracy: 0.3665\n",
      "Epoch 88/350\n",
      "89/89 [==============================] - 5s 55ms/step - loss: 2.5546 - accuracy: 0.3746\n",
      "Epoch 89/350\n",
      "89/89 [==============================] - 5s 56ms/step - loss: 2.5188 - accuracy: 0.3809\n",
      "Epoch 90/350\n",
      "89/89 [==============================] - 5s 55ms/step - loss: 2.4746 - accuracy: 0.3933\n",
      "Epoch 91/350\n",
      "89/89 [==============================] - 5s 55ms/step - loss: 2.4433 - accuracy: 0.3959\n",
      "Epoch 92/350\n",
      "89/89 [==============================] - 5s 54ms/step - loss: 2.5806 - accuracy: 0.3687\n",
      "Epoch 93/350\n",
      "89/89 [==============================] - 5s 53ms/step - loss: 2.5591 - accuracy: 0.3734\n",
      "Epoch 94/350\n",
      "89/89 [==============================] - 5s 53ms/step - loss: 2.4089 - accuracy: 0.4060\n",
      "Epoch 95/350\n",
      "89/89 [==============================] - 5s 53ms/step - loss: 2.3644 - accuracy: 0.4186\n",
      "Epoch 96/350\n",
      "89/89 [==============================] - 5s 53ms/step - loss: 2.4365 - accuracy: 0.3994\n",
      "Epoch 97/350\n",
      "89/89 [==============================] - 5s 52ms/step - loss: 2.5757 - accuracy: 0.3741\n",
      "Epoch 98/350\n",
      "89/89 [==============================] - 5s 53ms/step - loss: 2.3473 - accuracy: 0.4211\n",
      "Epoch 99/350\n",
      "89/89 [==============================] - 5s 53ms/step - loss: 2.2685 - accuracy: 0.4395\n",
      "Epoch 100/350\n",
      "89/89 [==============================] - 5s 54ms/step - loss: 2.2267 - accuracy: 0.4432\n",
      "Epoch 101/350\n",
      "89/89 [==============================] - 5s 53ms/step - loss: 2.2790 - accuracy: 0.4329\n",
      "Epoch 102/350\n",
      "89/89 [==============================] - 5s 53ms/step - loss: 2.4609 - accuracy: 0.4002\n",
      "Epoch 103/350\n",
      "89/89 [==============================] - 5s 54ms/step - loss: 2.2847 - accuracy: 0.4356\n",
      "Epoch 104/350\n",
      "89/89 [==============================] - 5s 54ms/step - loss: 2.2197 - accuracy: 0.4518\n",
      "Epoch 105/350\n",
      "89/89 [==============================] - 5s 54ms/step - loss: 2.1643 - accuracy: 0.4639\n",
      "Epoch 106/350\n",
      "89/89 [==============================] - 5s 54ms/step - loss: 2.1023 - accuracy: 0.4770\n",
      "Epoch 107/350\n",
      "89/89 [==============================] - 5s 54ms/step - loss: 2.0704 - accuracy: 0.4857\n",
      "Epoch 108/350\n",
      "89/89 [==============================] - 5s 55ms/step - loss: 2.3337 - accuracy: 0.4260\n",
      "Epoch 109/350\n",
      "89/89 [==============================] - 5s 54ms/step - loss: 2.2711 - accuracy: 0.4351\n",
      "Epoch 110/350\n",
      "89/89 [==============================] - 5s 54ms/step - loss: 2.2016 - accuracy: 0.4523\n",
      "Epoch 111/350\n",
      "89/89 [==============================] - 5s 54ms/step - loss: 2.1174 - accuracy: 0.4761\n",
      "Epoch 112/350\n",
      "89/89 [==============================] - 5s 54ms/step - loss: 2.1045 - accuracy: 0.4779\n",
      "Epoch 113/350\n",
      "89/89 [==============================] - 5s 54ms/step - loss: 2.0712 - accuracy: 0.4825\n",
      "Epoch 114/350\n",
      "89/89 [==============================] - 5s 54ms/step - loss: 2.0788 - accuracy: 0.4833\n",
      "Epoch 115/350\n",
      "89/89 [==============================] - 5s 54ms/step - loss: 2.0718 - accuracy: 0.4790\n",
      "Epoch 116/350\n",
      "89/89 [==============================] - 5s 55ms/step - loss: 2.0586 - accuracy: 0.4882\n",
      "Epoch 117/350\n",
      "89/89 [==============================] - 5s 54ms/step - loss: 1.9847 - accuracy: 0.5036\n",
      "Epoch 118/350\n",
      "89/89 [==============================] - 5s 54ms/step - loss: 1.8984 - accuracy: 0.5166\n",
      "Epoch 119/350\n",
      "89/89 [==============================] - 5s 57ms/step - loss: 1.8352 - accuracy: 0.5324\n",
      "Epoch 120/350\n",
      "89/89 [==============================] - 5s 57ms/step - loss: 1.7785 - accuracy: 0.5538\n",
      "Epoch 121/350\n",
      "89/89 [==============================] - 5s 57ms/step - loss: 1.7241 - accuracy: 0.5690\n",
      "Epoch 122/350\n",
      "89/89 [==============================] - 5s 54ms/step - loss: 1.8844 - accuracy: 0.5286\n",
      "Epoch 123/350\n",
      "89/89 [==============================] - 5s 58ms/step - loss: 2.3613 - accuracy: 0.4296\n",
      "Epoch 124/350\n",
      "89/89 [==============================] - 5s 56ms/step - loss: 2.2658 - accuracy: 0.4365\n",
      "Epoch 125/350\n",
      "89/89 [==============================] - 5s 56ms/step - loss: 2.1831 - accuracy: 0.4576\n",
      "Epoch 126/350\n",
      "89/89 [==============================] - 5s 60ms/step - loss: 2.1293 - accuracy: 0.4644\n",
      "Epoch 127/350\n",
      "89/89 [==============================] - 5s 57ms/step - loss: 2.0815 - accuracy: 0.4742\n",
      "Epoch 128/350\n",
      "89/89 [==============================] - 5s 55ms/step - loss: 2.0468 - accuracy: 0.4793\n",
      "Epoch 129/350\n",
      "89/89 [==============================] - 5s 55ms/step - loss: 2.0075 - accuracy: 0.4871\n",
      "Epoch 130/350\n",
      "89/89 [==============================] - 5s 54ms/step - loss: 1.9739 - accuracy: 0.4981\n",
      "Epoch 131/350\n",
      "89/89 [==============================] - 5s 53ms/step - loss: 1.9452 - accuracy: 0.5070\n",
      "Epoch 132/350\n",
      "89/89 [==============================] - 5s 53ms/step - loss: 1.9181 - accuracy: 0.5074\n",
      "Epoch 133/350\n",
      "89/89 [==============================] - 5s 53ms/step - loss: 1.8829 - accuracy: 0.5215\n",
      "Epoch 134/350\n",
      "89/89 [==============================] - 5s 53ms/step - loss: 1.8613 - accuracy: 0.5235\n",
      "Epoch 135/350\n",
      "89/89 [==============================] - 5s 53ms/step - loss: 1.8271 - accuracy: 0.5329\n",
      "Epoch 136/350\n",
      "89/89 [==============================] - 5s 54ms/step - loss: 1.8110 - accuracy: 0.5373\n",
      "Epoch 137/350\n",
      "89/89 [==============================] - 5s 54ms/step - loss: 1.7852 - accuracy: 0.5430\n",
      "Epoch 138/350\n",
      "89/89 [==============================] - 5s 55ms/step - loss: 1.7630 - accuracy: 0.5438\n",
      "Epoch 139/350\n",
      "89/89 [==============================] - 5s 55ms/step - loss: 1.7273 - accuracy: 0.5559\n",
      "Epoch 140/350\n",
      "89/89 [==============================] - 5s 55ms/step - loss: 1.7204 - accuracy: 0.5597\n",
      "Epoch 141/350\n",
      "89/89 [==============================] - 5s 55ms/step - loss: 1.6937 - accuracy: 0.5613\n",
      "Epoch 142/350\n",
      "89/89 [==============================] - 5s 55ms/step - loss: 1.6643 - accuracy: 0.5705\n",
      "Epoch 143/350\n",
      "89/89 [==============================] - 5s 55ms/step - loss: 1.6452 - accuracy: 0.5777\n",
      "Epoch 144/350\n",
      "89/89 [==============================] - 5s 54ms/step - loss: 1.6222 - accuracy: 0.5820\n",
      "Epoch 145/350\n",
      "89/89 [==============================] - 5s 54ms/step - loss: 1.5960 - accuracy: 0.5887\n",
      "Epoch 146/350\n",
      "89/89 [==============================] - 5s 54ms/step - loss: 1.5764 - accuracy: 0.5913\n",
      "Epoch 147/350\n",
      "89/89 [==============================] - 5s 53ms/step - loss: 1.5568 - accuracy: 0.5957\n",
      "Epoch 148/350\n",
      "89/89 [==============================] - 5s 53ms/step - loss: 1.5409 - accuracy: 0.6029\n",
      "Epoch 149/350\n",
      "89/89 [==============================] - 5s 54ms/step - loss: 1.5062 - accuracy: 0.6074\n",
      "Epoch 150/350\n",
      "89/89 [==============================] - 5s 54ms/step - loss: 1.4896 - accuracy: 0.6120\n",
      "Epoch 151/350\n",
      "89/89 [==============================] - 5s 55ms/step - loss: 1.4605 - accuracy: 0.6218\n",
      "Epoch 152/350\n",
      "89/89 [==============================] - 5s 54ms/step - loss: 1.4479 - accuracy: 0.6200\n",
      "Epoch 153/350\n",
      "89/89 [==============================] - 5s 55ms/step - loss: 1.4248 - accuracy: 0.6265\n",
      "Epoch 154/350\n",
      "89/89 [==============================] - 5s 54ms/step - loss: 1.4096 - accuracy: 0.6339\n",
      "Epoch 155/350\n",
      "89/89 [==============================] - 5s 54ms/step - loss: 1.3893 - accuracy: 0.6334\n",
      "Epoch 156/350\n",
      "89/89 [==============================] - 5s 54ms/step - loss: 1.3648 - accuracy: 0.6465\n",
      "Epoch 157/350\n",
      "89/89 [==============================] - 5s 54ms/step - loss: 1.3444 - accuracy: 0.6499\n",
      "Epoch 158/350\n",
      "89/89 [==============================] - 5s 54ms/step - loss: 1.3292 - accuracy: 0.6521\n",
      "Epoch 159/350\n",
      "89/89 [==============================] - 5s 54ms/step - loss: 1.3052 - accuracy: 0.6624\n",
      "Epoch 160/350\n",
      "89/89 [==============================] - 5s 55ms/step - loss: 1.2834 - accuracy: 0.6643\n",
      "Epoch 161/350\n",
      "89/89 [==============================] - 5s 54ms/step - loss: 1.2761 - accuracy: 0.6711\n",
      "Epoch 162/350\n",
      "89/89 [==============================] - 5s 54ms/step - loss: 1.2552 - accuracy: 0.6763\n",
      "Epoch 163/350\n",
      "89/89 [==============================] - 5s 54ms/step - loss: 1.2444 - accuracy: 0.6728\n",
      "Epoch 164/350\n",
      "89/89 [==============================] - 5s 54ms/step - loss: 1.2156 - accuracy: 0.6854\n",
      "Epoch 165/350\n",
      "89/89 [==============================] - 5s 56ms/step - loss: 1.2062 - accuracy: 0.6869\n",
      "Epoch 166/350\n",
      "89/89 [==============================] - 5s 54ms/step - loss: 1.1887 - accuracy: 0.6918\n",
      "Epoch 167/350\n",
      "89/89 [==============================] - 5s 54ms/step - loss: 1.1712 - accuracy: 0.6969\n",
      "Epoch 168/350\n",
      "89/89 [==============================] - 5s 54ms/step - loss: 1.1374 - accuracy: 0.7077\n",
      "Epoch 169/350\n",
      "89/89 [==============================] - 5s 54ms/step - loss: 1.1082 - accuracy: 0.7161\n",
      "Epoch 170/350\n",
      "89/89 [==============================] - 5s 54ms/step - loss: 1.0979 - accuracy: 0.7174\n",
      "Epoch 171/350\n",
      "89/89 [==============================] - 5s 54ms/step - loss: 1.0826 - accuracy: 0.7234\n",
      "Epoch 172/350\n",
      "89/89 [==============================] - 5s 54ms/step - loss: 1.0725 - accuracy: 0.7221\n",
      "Epoch 173/350\n",
      "89/89 [==============================] - 5s 54ms/step - loss: 1.0525 - accuracy: 0.7352\n",
      "Epoch 174/350\n",
      "89/89 [==============================] - 5s 55ms/step - loss: 1.0349 - accuracy: 0.7359\n",
      "Epoch 175/350\n",
      "89/89 [==============================] - 5s 54ms/step - loss: 1.0180 - accuracy: 0.7410\n",
      "Epoch 176/350\n",
      "89/89 [==============================] - 5s 54ms/step - loss: 0.9998 - accuracy: 0.7448\n",
      "Epoch 177/350\n",
      "89/89 [==============================] - 5s 56ms/step - loss: 0.9873 - accuracy: 0.7498\n",
      "Epoch 178/350\n",
      "89/89 [==============================] - 5s 54ms/step - loss: 0.9717 - accuracy: 0.7560\n",
      "Epoch 179/350\n",
      "89/89 [==============================] - 5s 53ms/step - loss: 0.9596 - accuracy: 0.7533\n",
      "Epoch 180/350\n",
      "89/89 [==============================] - 5s 54ms/step - loss: 0.9396 - accuracy: 0.7661\n",
      "Epoch 181/350\n",
      "89/89 [==============================] - 5s 53ms/step - loss: 0.9184 - accuracy: 0.7710\n",
      "Epoch 182/350\n",
      "89/89 [==============================] - 5s 53ms/step - loss: 0.9075 - accuracy: 0.7740\n",
      "Epoch 183/350\n",
      "89/89 [==============================] - 5s 54ms/step - loss: 0.8961 - accuracy: 0.7752\n",
      "Epoch 184/350\n",
      "89/89 [==============================] - 5s 55ms/step - loss: 0.8753 - accuracy: 0.7850\n",
      "Epoch 185/350\n",
      "89/89 [==============================] - 5s 56ms/step - loss: 0.8569 - accuracy: 0.7868\n",
      "Epoch 186/350\n",
      "89/89 [==============================] - 5s 55ms/step - loss: 0.8309 - accuracy: 0.7958\n",
      "Epoch 187/350\n",
      "89/89 [==============================] - 5s 56ms/step - loss: 0.8212 - accuracy: 0.8006\n",
      "Epoch 188/350\n",
      "89/89 [==============================] - 5s 55ms/step - loss: 0.8269 - accuracy: 0.7959\n",
      "Epoch 189/350\n",
      "89/89 [==============================] - 5s 57ms/step - loss: 0.8115 - accuracy: 0.8012\n",
      "Epoch 190/350\n",
      "89/89 [==============================] - 5s 54ms/step - loss: 0.8015 - accuracy: 0.8034\n",
      "Epoch 191/350\n",
      "89/89 [==============================] - 5s 53ms/step - loss: 0.7846 - accuracy: 0.8053\n",
      "Epoch 192/350\n",
      "89/89 [==============================] - 5s 53ms/step - loss: 0.7676 - accuracy: 0.8115\n",
      "Epoch 193/350\n",
      "89/89 [==============================] - 5s 53ms/step - loss: 0.7566 - accuracy: 0.8130\n",
      "Epoch 194/350\n",
      "89/89 [==============================] - 5s 53ms/step - loss: 0.7265 - accuracy: 0.8232\n",
      "Epoch 195/350\n",
      "89/89 [==============================] - 5s 53ms/step - loss: 0.6996 - accuracy: 0.8312\n",
      "Epoch 196/350\n",
      "89/89 [==============================] - 5s 53ms/step - loss: 0.6929 - accuracy: 0.8343\n",
      "Epoch 197/350\n",
      "89/89 [==============================] - 5s 54ms/step - loss: 0.6747 - accuracy: 0.8417\n",
      "Epoch 198/350\n",
      "89/89 [==============================] - 5s 53ms/step - loss: 0.6624 - accuracy: 0.8447\n",
      "Epoch 199/350\n",
      "89/89 [==============================] - 5s 54ms/step - loss: 0.6625 - accuracy: 0.8411\n",
      "Epoch 200/350\n",
      "89/89 [==============================] - 5s 55ms/step - loss: 0.6440 - accuracy: 0.8494\n",
      "Epoch 201/350\n",
      "89/89 [==============================] - 5s 55ms/step - loss: 0.6351 - accuracy: 0.8483\n",
      "Epoch 202/350\n",
      "89/89 [==============================] - 5s 56ms/step - loss: 0.6430 - accuracy: 0.8472\n",
      "Epoch 203/350\n",
      "89/89 [==============================] - 5s 56ms/step - loss: 0.6250 - accuracy: 0.8544\n",
      "Epoch 204/350\n",
      "89/89 [==============================] - 5s 56ms/step - loss: 0.6280 - accuracy: 0.8516\n",
      "Epoch 205/350\n",
      "89/89 [==============================] - 5s 55ms/step - loss: 0.5885 - accuracy: 0.8598\n",
      "Epoch 206/350\n",
      "89/89 [==============================] - 5s 54ms/step - loss: 0.5659 - accuracy: 0.8731\n",
      "Epoch 207/350\n",
      "89/89 [==============================] - 5s 54ms/step - loss: 0.5577 - accuracy: 0.8729\n",
      "Epoch 208/350\n",
      "89/89 [==============================] - 5s 53ms/step - loss: 0.5527 - accuracy: 0.8735\n",
      "Epoch 209/350\n",
      "89/89 [==============================] - 5s 53ms/step - loss: 0.5420 - accuracy: 0.8769\n",
      "Epoch 210/350\n",
      "89/89 [==============================] - 5s 53ms/step - loss: 0.5305 - accuracy: 0.8763\n",
      "Epoch 211/350\n",
      "89/89 [==============================] - 5s 54ms/step - loss: 0.5303 - accuracy: 0.8780\n",
      "Epoch 212/350\n",
      "89/89 [==============================] - 5s 53ms/step - loss: 0.4974 - accuracy: 0.8866\n",
      "Epoch 213/350\n",
      "89/89 [==============================] - 5s 54ms/step - loss: 0.4828 - accuracy: 0.8924\n",
      "Epoch 214/350\n",
      "89/89 [==============================] - 5s 54ms/step - loss: 0.4693 - accuracy: 0.8969\n",
      "Epoch 215/350\n",
      "89/89 [==============================] - 5s 55ms/step - loss: 0.4657 - accuracy: 0.8975\n",
      "Epoch 216/350\n",
      "89/89 [==============================] - 5s 55ms/step - loss: 0.4624 - accuracy: 0.8979\n",
      "Epoch 217/350\n",
      "89/89 [==============================] - 5s 54ms/step - loss: 0.4618 - accuracy: 0.8982\n",
      "Epoch 218/350\n",
      "89/89 [==============================] - 5s 55ms/step - loss: 0.4430 - accuracy: 0.9013\n",
      "Epoch 219/350\n",
      "89/89 [==============================] - 5s 55ms/step - loss: 0.4314 - accuracy: 0.9087\n",
      "Epoch 220/350\n",
      "89/89 [==============================] - 5s 55ms/step - loss: 0.4380 - accuracy: 0.9026\n",
      "Epoch 221/350\n",
      "89/89 [==============================] - 5s 55ms/step - loss: 0.4402 - accuracy: 0.9022\n",
      "Epoch 222/350\n",
      "89/89 [==============================] - 5s 55ms/step - loss: 0.4140 - accuracy: 0.9097\n",
      "Epoch 223/350\n",
      "89/89 [==============================] - 5s 55ms/step - loss: 0.3927 - accuracy: 0.9188\n",
      "Epoch 224/350\n",
      "89/89 [==============================] - 5s 55ms/step - loss: 0.3710 - accuracy: 0.9233\n",
      "Epoch 225/350\n",
      "89/89 [==============================] - 5s 55ms/step - loss: 0.3530 - accuracy: 0.9293\n",
      "Epoch 226/350\n",
      "89/89 [==============================] - 5s 55ms/step - loss: 0.3356 - accuracy: 0.9354\n",
      "Epoch 227/350\n",
      "89/89 [==============================] - 5s 55ms/step - loss: 0.3414 - accuracy: 0.9317\n",
      "Epoch 228/350\n",
      "89/89 [==============================] - 5s 55ms/step - loss: 0.3147 - accuracy: 0.9406\n",
      "Epoch 229/350\n",
      "89/89 [==============================] - 5s 55ms/step - loss: 0.3114 - accuracy: 0.9404\n",
      "Epoch 230/350\n",
      "89/89 [==============================] - 5s 57ms/step - loss: 0.3086 - accuracy: 0.9432\n",
      "Epoch 231/350\n",
      "89/89 [==============================] - 5s 57ms/step - loss: 0.2921 - accuracy: 0.9461\n",
      "Epoch 232/350\n",
      "89/89 [==============================] - 5s 55ms/step - loss: 0.2936 - accuracy: 0.9455\n",
      "Epoch 233/350\n",
      "89/89 [==============================] - 5s 55ms/step - loss: 0.2985 - accuracy: 0.9455\n",
      "Epoch 234/350\n",
      "89/89 [==============================] - 5s 55ms/step - loss: 0.2985 - accuracy: 0.9452\n",
      "Epoch 235/350\n",
      "89/89 [==============================] - 5s 54ms/step - loss: 0.3125 - accuracy: 0.9389\n",
      "Epoch 236/350\n",
      "89/89 [==============================] - 5s 53ms/step - loss: 0.3102 - accuracy: 0.9391\n",
      "Epoch 237/350\n",
      "89/89 [==============================] - 5s 53ms/step - loss: 0.2765 - accuracy: 0.9482\n",
      "Epoch 238/350\n",
      "89/89 [==============================] - 5s 54ms/step - loss: 0.2621 - accuracy: 0.9538\n",
      "Epoch 239/350\n",
      "89/89 [==============================] - 5s 53ms/step - loss: 0.2672 - accuracy: 0.9488\n",
      "Epoch 240/350\n",
      "89/89 [==============================] - 5s 57ms/step - loss: 0.2509 - accuracy: 0.9560\n",
      "Epoch 241/350\n",
      "89/89 [==============================] - 5s 55ms/step - loss: 0.2909 - accuracy: 0.9413\n",
      "Epoch 242/350\n",
      "89/89 [==============================] - 5s 54ms/step - loss: 0.2793 - accuracy: 0.9435\n",
      "Epoch 243/350\n",
      "89/89 [==============================] - 5s 53ms/step - loss: 0.2843 - accuracy: 0.9427\n",
      "Epoch 244/350\n",
      "89/89 [==============================] - 5s 53ms/step - loss: 0.2540 - accuracy: 0.9524\n",
      "Epoch 245/350\n",
      "89/89 [==============================] - 5s 53ms/step - loss: 0.2280 - accuracy: 0.9612\n",
      "Epoch 246/350\n",
      "89/89 [==============================] - 5s 53ms/step - loss: 0.2012 - accuracy: 0.9688\n",
      "Epoch 247/350\n",
      "89/89 [==============================] - 5s 53ms/step - loss: 0.1782 - accuracy: 0.9729\n",
      "Epoch 248/350\n",
      "89/89 [==============================] - 5s 53ms/step - loss: 0.1618 - accuracy: 0.9775\n",
      "Epoch 249/350\n",
      "89/89 [==============================] - 5s 54ms/step - loss: 0.1528 - accuracy: 0.9798\n",
      "Epoch 250/350\n",
      "89/89 [==============================] - 5s 55ms/step - loss: 0.1466 - accuracy: 0.9821\n",
      "Epoch 251/350\n",
      "89/89 [==============================] - 5s 56ms/step - loss: 0.1445 - accuracy: 0.9819\n",
      "Epoch 252/350\n",
      "89/89 [==============================] - 5s 56ms/step - loss: 0.1430 - accuracy: 0.9830\n",
      "Epoch 253/350\n",
      "89/89 [==============================] - 5s 55ms/step - loss: 0.1460 - accuracy: 0.9816\n",
      "Epoch 254/350\n",
      "89/89 [==============================] - 5s 56ms/step - loss: 0.1578 - accuracy: 0.9786\n",
      "Epoch 255/350\n",
      "89/89 [==============================] - 5s 55ms/step - loss: 0.1892 - accuracy: 0.9692\n",
      "Epoch 256/350\n",
      "89/89 [==============================] - 5s 54ms/step - loss: 0.3402 - accuracy: 0.9125\n",
      "Epoch 257/350\n",
      "89/89 [==============================] - 5s 54ms/step - loss: 0.5316 - accuracy: 0.8458\n",
      "Epoch 258/350\n",
      "89/89 [==============================] - 5s 53ms/step - loss: 0.4130 - accuracy: 0.8866\n",
      "Epoch 259/350\n",
      "89/89 [==============================] - 5s 56ms/step - loss: 0.2295 - accuracy: 0.9524\n",
      "Epoch 260/350\n",
      "89/89 [==============================] - 5s 54ms/step - loss: 0.1492 - accuracy: 0.9805\n",
      "Epoch 261/350\n",
      "89/89 [==============================] - 5s 54ms/step - loss: 0.1212 - accuracy: 0.9859\n",
      "Epoch 262/350\n",
      "89/89 [==============================] - 5s 54ms/step - loss: 0.1029 - accuracy: 0.9896\n",
      "Epoch 263/350\n",
      "89/89 [==============================] - 5s 55ms/step - loss: 0.0945 - accuracy: 0.9909\n",
      "Epoch 264/350\n",
      "89/89 [==============================] - 5s 55ms/step - loss: 0.0882 - accuracy: 0.9924\n",
      "Epoch 265/350\n",
      "89/89 [==============================] - 5s 55ms/step - loss: 0.0849 - accuracy: 0.9935\n",
      "Epoch 266/350\n",
      "89/89 [==============================] - 5s 55ms/step - loss: 0.0824 - accuracy: 0.9933\n",
      "Epoch 267/350\n",
      "89/89 [==============================] - 5s 56ms/step - loss: 0.0784 - accuracy: 0.9931\n",
      "Epoch 268/350\n",
      "89/89 [==============================] - 5s 55ms/step - loss: 0.0757 - accuracy: 0.9943\n",
      "Epoch 269/350\n",
      "89/89 [==============================] - 5s 55ms/step - loss: 0.0731 - accuracy: 0.9952\n",
      "Epoch 270/350\n",
      "89/89 [==============================] - 5s 54ms/step - loss: 0.0714 - accuracy: 0.9952\n",
      "Epoch 271/350\n",
      "89/89 [==============================] - 5s 53ms/step - loss: 0.0683 - accuracy: 0.9963\n",
      "Epoch 272/350\n",
      "89/89 [==============================] - 5s 53ms/step - loss: 0.0661 - accuracy: 0.9964\n",
      "Epoch 273/350\n",
      "89/89 [==============================] - 5s 55ms/step - loss: 0.0637 - accuracy: 0.9965\n",
      "Epoch 274/350\n",
      "89/89 [==============================] - 5s 53ms/step - loss: 0.0627 - accuracy: 0.9969\n",
      "Epoch 275/350\n",
      "89/89 [==============================] - 5s 54ms/step - loss: 0.0621 - accuracy: 0.9968\n",
      "Epoch 276/350\n",
      "89/89 [==============================] - 5s 53ms/step - loss: 0.0612 - accuracy: 0.9967\n",
      "Epoch 277/350\n",
      "89/89 [==============================] - 5s 53ms/step - loss: 0.0593 - accuracy: 0.9969\n",
      "Epoch 278/350\n",
      "89/89 [==============================] - 5s 54ms/step - loss: 0.0577 - accuracy: 0.9972\n",
      "Epoch 279/350\n",
      "89/89 [==============================] - 5s 55ms/step - loss: 0.0608 - accuracy: 0.9967\n",
      "Epoch 280/350\n",
      "89/89 [==============================] - 5s 56ms/step - loss: 0.0600 - accuracy: 0.9973\n",
      "Epoch 281/350\n",
      "89/89 [==============================] - 5s 55ms/step - loss: 0.0672 - accuracy: 0.9961\n",
      "Epoch 282/350\n",
      "89/89 [==============================] - 5s 55ms/step - loss: 0.2788 - accuracy: 0.9262\n",
      "Epoch 283/350\n",
      "89/89 [==============================] - 5s 55ms/step - loss: 1.0321 - accuracy: 0.7141\n",
      "Epoch 284/350\n",
      "89/89 [==============================] - 5s 55ms/step - loss: 0.5346 - accuracy: 0.8380\n",
      "Epoch 285/350\n",
      "89/89 [==============================] - 5s 55ms/step - loss: 0.2045 - accuracy: 0.9524\n",
      "Epoch 286/350\n",
      "89/89 [==============================] - 5s 55ms/step - loss: 0.1048 - accuracy: 0.9854\n",
      "Epoch 287/350\n",
      "89/89 [==============================] - 5s 55ms/step - loss: 0.0647 - accuracy: 0.9971\n",
      "Epoch 288/350\n",
      "89/89 [==============================] - 5s 55ms/step - loss: 0.0500 - accuracy: 0.9984\n",
      "Epoch 289/350\n",
      "89/89 [==============================] - 5s 55ms/step - loss: 0.0452 - accuracy: 0.9985\n",
      "Epoch 290/350\n",
      "89/89 [==============================] - 5s 55ms/step - loss: 0.0416 - accuracy: 0.9989\n",
      "Epoch 291/350\n",
      "89/89 [==============================] - 5s 55ms/step - loss: 0.0390 - accuracy: 0.9993\n",
      "Epoch 292/350\n",
      "89/89 [==============================] - 5s 55ms/step - loss: 0.0368 - accuracy: 0.9995\n",
      "Epoch 293/350\n",
      "89/89 [==============================] - 5s 55ms/step - loss: 0.0352 - accuracy: 0.9994\n",
      "Epoch 294/350\n",
      "89/89 [==============================] - 5s 55ms/step - loss: 0.0336 - accuracy: 0.9995\n",
      "Epoch 295/350\n",
      "89/89 [==============================] - 5s 55ms/step - loss: 0.0319 - accuracy: 0.9998\n",
      "Epoch 296/350\n",
      "89/89 [==============================] - 5s 55ms/step - loss: 0.0310 - accuracy: 0.9998\n",
      "Epoch 297/350\n",
      "89/89 [==============================] - 5s 55ms/step - loss: 0.0300 - accuracy: 0.9996\n",
      "Epoch 298/350\n",
      "89/89 [==============================] - 5s 55ms/step - loss: 0.0287 - accuracy: 0.9997\n",
      "Epoch 299/350\n",
      "89/89 [==============================] - 5s 55ms/step - loss: 0.0277 - accuracy: 1.0000\n",
      "Epoch 300/350\n",
      "89/89 [==============================] - 5s 54ms/step - loss: 0.0264 - accuracy: 1.0000\n",
      "Epoch 301/350\n",
      "89/89 [==============================] - 5s 54ms/step - loss: 0.0263 - accuracy: 0.9996\n",
      "Epoch 302/350\n",
      "89/89 [==============================] - 5s 53ms/step - loss: 0.0256 - accuracy: 0.9998\n",
      "Epoch 303/350\n",
      "89/89 [==============================] - 5s 53ms/step - loss: 0.0243 - accuracy: 0.9999\n",
      "Epoch 304/350\n",
      "89/89 [==============================] - 5s 54ms/step - loss: 0.0234 - accuracy: 0.9998\n",
      "Epoch 305/350\n",
      "89/89 [==============================] - 5s 53ms/step - loss: 0.0225 - accuracy: 1.0000\n",
      "Epoch 306/350\n",
      "89/89 [==============================] - 5s 53ms/step - loss: 0.0219 - accuracy: 0.9999\n",
      "Epoch 307/350\n",
      "89/89 [==============================] - 5s 54ms/step - loss: 0.0211 - accuracy: 1.0000\n",
      "Epoch 308/350\n",
      "89/89 [==============================] - 5s 56ms/step - loss: 0.0204 - accuracy: 1.0000\n",
      "Epoch 309/350\n",
      "89/89 [==============================] - 5s 56ms/step - loss: 0.0196 - accuracy: 0.9999\n",
      "Epoch 310/350\n",
      "89/89 [==============================] - 5s 56ms/step - loss: 0.0188 - accuracy: 1.0000\n",
      "Epoch 311/350\n",
      "89/89 [==============================] - 5s 55ms/step - loss: 0.0200 - accuracy: 0.9997\n",
      "Epoch 312/350\n",
      "89/89 [==============================] - 5s 56ms/step - loss: 0.0193 - accuracy: 0.9998\n",
      "Epoch 313/350\n",
      "89/89 [==============================] - 5s 56ms/step - loss: 0.0353 - accuracy: 0.9980\n",
      "Epoch 314/350\n",
      "89/89 [==============================] - 5s 54ms/step - loss: 0.5719 - accuracy: 0.8473\n",
      "Epoch 315/350\n",
      "89/89 [==============================] - 5s 54ms/step - loss: 1.4840 - accuracy: 0.6870\n",
      "Epoch 316/350\n",
      "89/89 [==============================] - 5s 53ms/step - loss: 0.3865 - accuracy: 0.8877\n",
      "Epoch 317/350\n",
      "89/89 [==============================] - 5s 54ms/step - loss: 0.1185 - accuracy: 0.9769\n",
      "Epoch 318/350\n",
      "89/89 [==============================] - 5s 53ms/step - loss: 0.0589 - accuracy: 0.9950\n",
      "Epoch 319/350\n",
      "89/89 [==============================] - 5s 53ms/step - loss: 0.0353 - accuracy: 0.9996\n",
      "Epoch 320/350\n",
      "89/89 [==============================] - 5s 58ms/step - loss: 0.0278 - accuracy: 0.9998\n",
      "Epoch 321/350\n",
      "89/89 [==============================] - 5s 58ms/step - loss: 0.0251 - accuracy: 0.9997\n",
      "Epoch 322/350\n",
      "89/89 [==============================] - 5s 57ms/step - loss: 0.0229 - accuracy: 0.9999\n",
      "Epoch 323/350\n",
      "89/89 [==============================] - 5s 54ms/step - loss: 0.0217 - accuracy: 1.0000\n",
      "Epoch 324/350\n",
      "89/89 [==============================] - 5s 55ms/step - loss: 0.0201 - accuracy: 1.0000\n",
      "Epoch 325/350\n",
      "89/89 [==============================] - 5s 56ms/step - loss: 0.0193 - accuracy: 1.0000\n",
      "Epoch 326/350\n",
      "89/89 [==============================] - 5s 56ms/step - loss: 0.0182 - accuracy: 1.0000\n",
      "Epoch 327/350\n",
      "89/89 [==============================] - 5s 56ms/step - loss: 0.0175 - accuracy: 1.0000\n",
      "Epoch 328/350\n",
      "89/89 [==============================] - 5s 56ms/step - loss: 0.0168 - accuracy: 1.0000\n",
      "Epoch 329/350\n",
      "89/89 [==============================] - 5s 56ms/step - loss: 0.0161 - accuracy: 1.0000\n",
      "Epoch 330/350\n",
      "89/89 [==============================] - 5s 56ms/step - loss: 0.0156 - accuracy: 1.0000\n",
      "Epoch 331/350\n",
      "89/89 [==============================] - 5s 55ms/step - loss: 0.0150 - accuracy: 1.0000\n",
      "Epoch 332/350\n",
      "89/89 [==============================] - 5s 54ms/step - loss: 0.0144 - accuracy: 1.0000\n",
      "Epoch 333/350\n",
      "89/89 [==============================] - 5s 54ms/step - loss: 0.0140 - accuracy: 1.0000\n",
      "Epoch 334/350\n",
      "89/89 [==============================] - 5s 54ms/step - loss: 0.0136 - accuracy: 1.0000\n",
      "Epoch 335/350\n",
      "89/89 [==============================] - 5s 54ms/step - loss: 0.0131 - accuracy: 1.0000\n",
      "Epoch 336/350\n",
      "89/89 [==============================] - 5s 55ms/step - loss: 0.0127 - accuracy: 1.0000\n",
      "Epoch 337/350\n",
      "89/89 [==============================] - 5s 54ms/step - loss: 0.0123 - accuracy: 1.0000\n",
      "Epoch 338/350\n",
      "89/89 [==============================] - 5s 54ms/step - loss: 0.0120 - accuracy: 1.0000\n",
      "Epoch 339/350\n",
      "89/89 [==============================] - 5s 54ms/step - loss: 0.0116 - accuracy: 1.0000\n",
      "Epoch 340/350\n",
      "89/89 [==============================] - 5s 55ms/step - loss: 0.0112 - accuracy: 1.0000\n",
      "Epoch 341/350\n",
      "89/89 [==============================] - 5s 55ms/step - loss: 0.0109 - accuracy: 1.0000\n",
      "Epoch 342/350\n",
      "89/89 [==============================] - 5s 55ms/step - loss: 0.0106 - accuracy: 1.0000\n",
      "Epoch 343/350\n",
      "89/89 [==============================] - 5s 56ms/step - loss: 0.0102 - accuracy: 1.0000\n",
      "Epoch 344/350\n",
      "89/89 [==============================] - 5s 55ms/step - loss: 0.0100 - accuracy: 1.0000\n",
      "Epoch 345/350\n",
      "89/89 [==============================] - 5s 56ms/step - loss: 0.0097 - accuracy: 1.0000\n",
      "Epoch 346/350\n",
      "89/89 [==============================] - 5s 56ms/step - loss: 0.0095 - accuracy: 1.0000\n",
      "Epoch 347/350\n",
      "89/89 [==============================] - 5s 56ms/step - loss: 0.0091 - accuracy: 1.0000\n",
      "Epoch 348/350\n",
      "89/89 [==============================] - 5s 55ms/step - loss: 0.0088 - accuracy: 1.0000\n",
      "Epoch 349/350\n",
      "89/89 [==============================] - 5s 56ms/step - loss: 0.0085 - accuracy: 1.0000\n",
      "Epoch 350/350\n",
      "89/89 [==============================] - 5s 55ms/step - loss: 0.0083 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x15782274af0>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fitting the model\n",
    "\n",
    "model.fit(X, y, batch_size = 128, epochs = 350, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "514852ac-2e39-43fd-8578-7c0110d495f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the model to file\n",
    "\n",
    "model.save('epoch300.h5')\n",
    "\n",
    "# saving the tokenizer\n",
    "\n",
    "dump(tokenizer, open('epoch300', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db359914-5de7-4234-938c-b7d236713499",
   "metadata": {},
   "source": [
    "## Generating New Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1daf25f9-b868-4b0d-8374-fb2d5782bce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "from pickle import load\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e4e3363b-7a30-4176-b218-5b93d3ce990e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# going to take in a model, a tokenizer, seq_len, seed_text, num_gen_words\n",
    "\n",
    "def generate_text(model, tokenizer, seq_len, seed_text, num_gen_words):\n",
    "    '''\n",
    "    INPUTS:\n",
    "    model : model that was trained on text data\n",
    "    tokenizer : tokenizer that was fit on text data\n",
    "    seq_len : length of training sequence\n",
    "    seed_text : raw string text to serve as the seed\n",
    "    num_gen_words : number of words to be generated by model\n",
    "    '''\n",
    "    \n",
    "    # Final Output\n",
    "    output_text = []\n",
    "    \n",
    "    # Intial Seed Sequence\n",
    "    input_text = seed_text\n",
    "    \n",
    "    # Create num_gen_words\n",
    "    for i in range(num_gen_words):\n",
    "        \n",
    "        # Take the input text string and encode it to a sequence\n",
    "        encoded_text = tokenizer.texts_to_sequences([input_text])[0]\n",
    "        \n",
    "        # Pad sequences to our trained rate (50 words in the video)\n",
    "        pad_encoded = pad_sequences([encoded_text], maxlen=seq_len, truncating='pre')\n",
    "        \n",
    "        # Predict Class Probabilities for each word\n",
    "        pred_word_ind = np.argmax(model.predict(pad_encoded, verbose=0), axis = -1)[0]\n",
    "        \n",
    "        # Grab word\n",
    "        pred_word = tokenizer.index_word[pred_word_ind] \n",
    "        \n",
    "        # Update the sequence of input text (shifting one over with the new word)\n",
    "        input_text += ' ' + pred_word\n",
    "        \n",
    "        output_text.append(pred_word)\n",
    "        \n",
    "    # Make it look like a sentence.\n",
    "    return ' '.join(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fb5a56b6-c5ad-4613-9390-2a1420c36452",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['call',\n",
       " 'me',\n",
       " 'ishmael',\n",
       " 'some',\n",
       " 'years',\n",
       " 'ago',\n",
       " 'never',\n",
       " 'mind',\n",
       " 'how',\n",
       " 'long',\n",
       " 'precisely',\n",
       " 'having',\n",
       " 'little',\n",
       " 'or',\n",
       " 'no',\n",
       " 'money',\n",
       " 'in',\n",
       " 'my',\n",
       " 'purse',\n",
       " 'and',\n",
       " 'nothing',\n",
       " 'particular',\n",
       " 'to',\n",
       " 'interest',\n",
       " 'me',\n",
       " 'on']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_sequences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fe2b6cd5-0487-4c20-8556-fdd2dcc77566",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(101)\n",
    "random_pick = random.randint(0,len(text_sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e3f737ae-b15e-437e-aeba-0e76a7f6b711",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed_text = text_sequences[random_pick]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fa68d538-dc90-4d2b-83dd-725c235bad4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['thought',\n",
       " 'i',\n",
       " 'to',\n",
       " 'myself',\n",
       " 'the',\n",
       " 'man',\n",
       " \"'s\",\n",
       " 'a',\n",
       " 'human',\n",
       " 'being',\n",
       " 'just',\n",
       " 'as',\n",
       " 'i',\n",
       " 'am',\n",
       " 'he',\n",
       " 'has',\n",
       " 'just',\n",
       " 'as',\n",
       " 'much',\n",
       " 'reason',\n",
       " 'to',\n",
       " 'fear',\n",
       " 'me',\n",
       " 'as',\n",
       " 'i',\n",
       " 'have']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_seed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7a926e66-14b9-4e22-a408-0ef7c28340a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_text = ' '.join(random_seed_text) # joining the list of words to form a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c179c740-ab54-4d00-a20b-9fb65ece61fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"thought i to myself the man 's a human being just as i am he has just as much reason to fear me as i have\""
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9919e88d-bb73-464d-ac83-241e8fa56ec5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"to be afraid of him better sleep with a sober cannibal than a drunken christian landlord said i tell him to stash his tomahawk there or pipe or whatever you call it tell him to stop smoking in short and i will turn in with him but i do n't\""
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model,tokenizer,seq_len,seed_text=seed_text,num_gen_words=50) # generating the next 50 words"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
